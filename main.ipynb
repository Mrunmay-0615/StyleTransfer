{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMei3A2wUXlj",
        "outputId": "634fc773-a965-46db-9f51-f5b1ecd830fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-12 15:50:01--  https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/vangogh2photo.zip\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306590349 (292M) [application/zip]\n",
            "Saving to: ‘vangogh2photo.zip’\n",
            "\n",
            "vangogh2photo.zip   100%[===================>] 292.39M  63.9MB/s    in 4.9s    \n",
            "\n",
            "2023-11-12 15:50:06 (60.0 MB/s) - ‘vangogh2photo.zip’ saved [306590349/306590349]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Obtain the dataset\n",
        "!wget https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/vangogh2photo.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-ignite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxiJ5uKry7Xs",
        "outputId": "05322cb1-b73e-4937-8617-75b81fdd69e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.13-py3-none-any.whl (272 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/272.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m266.2/272.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.4/272.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (2.1.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite) (23.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1.3->pytorch-ignite) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eT34gv_CgJ0l"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "file = ZipFile('vangogh2photo.zip')\n",
        "file.extractall('drive/MyDrive')\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 by Gongfan Fang, Zhejiang University.\n",
        "# All rights reserved.\n",
        "import warnings\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def _fspecial_gauss_1d(size: int, sigma: float) -> Tensor:\n",
        "    r\"\"\"Create 1-D gauss kernel\n",
        "    Args:\n",
        "        size (int): the size of gauss kernel\n",
        "        sigma (float): sigma of normal distribution\n",
        "    Returns:\n",
        "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
        "    \"\"\"\n",
        "    coords = torch.arange(size, dtype=torch.float)\n",
        "    coords -= size // 2\n",
        "\n",
        "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
        "    g /= g.sum()\n",
        "\n",
        "    return g.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def gaussian_filter(input: Tensor, win: Tensor) -> Tensor:\n",
        "    r\"\"\" Blur input with 1-D kernel\n",
        "    Args:\n",
        "        input (torch.Tensor): a batch of tensors to be blurred\n",
        "        window (torch.Tensor): 1-D gauss kernel\n",
        "    Returns:\n",
        "        torch.Tensor: blurred tensors\n",
        "    \"\"\"\n",
        "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
        "    if len(input.shape) == 4:\n",
        "        conv = F.conv2d\n",
        "    elif len(input.shape) == 5:\n",
        "        conv = F.conv3d\n",
        "    else:\n",
        "        raise NotImplementedError(input.shape)\n",
        "\n",
        "    C = input.shape[1]\n",
        "    out = input\n",
        "    for i, s in enumerate(input.shape[2:]):\n",
        "        if s >= win.shape[-1]:\n",
        "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
        "        else:\n",
        "            warnings.warn(\n",
        "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
        "            )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float,\n",
        "    win: Tensor,\n",
        "    size_average: bool = True,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    r\"\"\" Calculate ssim index for X and Y\n",
        "\n",
        "    Args:\n",
        "        X (torch.Tensor): images\n",
        "        Y (torch.Tensor): images\n",
        "        data_range (float or int): value range of input images. (usually 1.0 or 255)\n",
        "        win (torch.Tensor): 1-D gauss kernel\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: ssim results.\n",
        "    \"\"\"\n",
        "    K1, K2 = K\n",
        "    # batch, channel, [depth,] height, width = X.shape\n",
        "    compensation = 1.0\n",
        "\n",
        "    C1 = (K1 * data_range) ** 2\n",
        "    C2 = (K2 * data_range) ** 2\n",
        "\n",
        "    win = win.to(X.device, dtype=X.dtype)\n",
        "\n",
        "    mu1 = gaussian_filter(X, win)\n",
        "    mu2 = gaussian_filter(Y, win)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
        "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
        "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
        "\n",
        "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
        "\n",
        "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
        "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
        "    return ssim_per_channel, cs\n",
        "\n",
        "\n",
        "def ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float = 255,\n",
        "    size_average: bool = True,\n",
        "    win_size: int = 11,\n",
        "    win_sigma: float = 1.5,\n",
        "    win: Optional[Tensor] = None,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "    nonnegative_ssim: bool = False,\n",
        ") -> Tensor:\n",
        "    r\"\"\" interface of ssim\n",
        "    Args:\n",
        "        X (torch.Tensor): a batch of images, (N,C,H,W)\n",
        "        Y (torch.Tensor): a batch of images, (N,C,H,W)\n",
        "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "        win_size: (int, optional): the size of gauss kernel\n",
        "        win_sigma: (float, optional): sigma of normal distribution\n",
        "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
        "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: ssim results\n",
        "    \"\"\"\n",
        "    if not X.shape == Y.shape:\n",
        "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
        "\n",
        "    for d in range(len(X.shape) - 1, 1, -1):\n",
        "        X = X.squeeze(dim=d)\n",
        "        Y = Y.squeeze(dim=d)\n",
        "\n",
        "    if len(X.shape) not in (4, 5):\n",
        "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
        "\n",
        "    #if not X.type() == Y.type():\n",
        "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
        "\n",
        "    if win is not None:  # set win_size\n",
        "        win_size = win.shape[-1]\n",
        "\n",
        "    if not (win_size % 2 == 1):\n",
        "        raise ValueError(\"Window size should be odd.\")\n",
        "\n",
        "    if win is None:\n",
        "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
        "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
        "\n",
        "    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n",
        "    if nonnegative_ssim:\n",
        "        ssim_per_channel = torch.relu(ssim_per_channel)\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_per_channel.mean()\n",
        "    else:\n",
        "        return ssim_per_channel.mean(1)\n",
        "\n",
        "\n",
        "def ms_ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float = 255,\n",
        "    size_average: bool = True,\n",
        "    win_size: int = 11,\n",
        "    win_sigma: float = 1.5,\n",
        "    win: Optional[Tensor] = None,\n",
        "    weights: Optional[List[float]] = None,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
        ") -> Tensor:\n",
        "    r\"\"\" interface of ms-ssim\n",
        "    Args:\n",
        "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
        "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
        "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "        win_size: (int, optional): the size of gauss kernel\n",
        "        win_sigma: (float, optional): sigma of normal distribution\n",
        "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
        "        weights (list, optional): weights for different levels\n",
        "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "    Returns:\n",
        "        torch.Tensor: ms-ssim results\n",
        "    \"\"\"\n",
        "    if not X.shape == Y.shape:\n",
        "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
        "\n",
        "    for d in range(len(X.shape) - 1, 1, -1):\n",
        "        X = X.squeeze(dim=d)\n",
        "        Y = Y.squeeze(dim=d)\n",
        "\n",
        "    #if not X.type() == Y.type():\n",
        "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
        "\n",
        "    if len(X.shape) == 4:\n",
        "        avg_pool = F.avg_pool2d\n",
        "    elif len(X.shape) == 5:\n",
        "        avg_pool = F.avg_pool3d\n",
        "    else:\n",
        "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
        "\n",
        "    if win is not None:  # set win_size\n",
        "        win_size = win.shape[-1]\n",
        "\n",
        "    if not (win_size % 2 == 1):\n",
        "        raise ValueError(\"Window size should be odd.\")\n",
        "\n",
        "    smaller_side = min(X.shape[-2:])\n",
        "    assert smaller_side > (win_size - 1) * (\n",
        "        2 ** 4\n",
        "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
        "\n",
        "    if weights is None:\n",
        "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
        "    weights_tensor = X.new_tensor(weights)\n",
        "\n",
        "    if win is None:\n",
        "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
        "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
        "\n",
        "    levels = weights_tensor.shape[0]\n",
        "    mcs = []\n",
        "    for i in range(levels):\n",
        "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n",
        "\n",
        "        if i < levels - 1:\n",
        "            mcs.append(torch.relu(cs))\n",
        "            padding = [s % 2 for s in X.shape[2:]]\n",
        "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
        "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
        "\n",
        "    ssim_per_channel = torch.relu(ssim_per_channel)  # type: ignore  # (batch, channel)\n",
        "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
        "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights_tensor.view(-1, 1, 1), dim=0)\n",
        "\n",
        "    if size_average:\n",
        "        return ms_ssim_val.mean()\n",
        "    else:\n",
        "        return ms_ssim_val.mean(1)\n",
        "\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_range: float = 255,\n",
        "        size_average: bool = True,\n",
        "        win_size: int = 11,\n",
        "        win_sigma: float = 1.5,\n",
        "        channel: int = 3,\n",
        "        spatial_dims: int = 2,\n",
        "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "        nonnegative_ssim: bool = False,\n",
        "    ) -> None:\n",
        "        r\"\"\" class for ssim\n",
        "        Args:\n",
        "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "            win_size: (int, optional): the size of gauss kernel\n",
        "            win_sigma: (float, optional): sigma of normal distribution\n",
        "            channel (int, optional): input channels (default: 3)\n",
        "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n",
        "        \"\"\"\n",
        "\n",
        "        super(SSIM, self).__init__()\n",
        "        self.win_size = win_size\n",
        "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.K = K\n",
        "        self.nonnegative_ssim = nonnegative_ssim\n",
        "\n",
        "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
        "        return ssim(\n",
        "            X,\n",
        "            Y,\n",
        "            data_range=self.data_range,\n",
        "            size_average=self.size_average,\n",
        "            win=self.win,\n",
        "            K=self.K,\n",
        "            nonnegative_ssim=self.nonnegative_ssim,\n",
        "        )\n",
        "\n",
        "\n",
        "class MS_SSIM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_range: float = 255,\n",
        "        size_average: bool = True,\n",
        "        win_size: int = 11,\n",
        "        win_sigma: float = 1.5,\n",
        "        channel: int = 3,\n",
        "        spatial_dims: int = 2,\n",
        "        weights: Optional[List[float]] = None,\n",
        "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "    ) -> None:\n",
        "        r\"\"\" class for ms-ssim\n",
        "        Args:\n",
        "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "            win_size: (int, optional): the size of gauss kernel\n",
        "            win_sigma: (float, optional): sigma of normal distribution\n",
        "            channel (int, optional): input channels (default: 3)\n",
        "            weights (list, optional): weights for different levels\n",
        "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MS_SSIM, self).__init__()\n",
        "        self.win_size = win_size\n",
        "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.weights = weights\n",
        "        self.K = K\n",
        "\n",
        "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
        "        return ms_ssim(\n",
        "            X,\n",
        "            Y,\n",
        "            data_range=self.data_range,\n",
        "            size_average=self.size_average,\n",
        "            win=self.win,\n",
        "            weights=self.weights,\n",
        "            K=self.K,\n",
        "        )"
      ],
      "metadata": {
        "id": "pKJirIqfARAb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the model architecture.\n",
        "\n",
        "As stated in the Readme.md, I am using CycleGAN (Zhu et. al, 2017) for transferring the Vangogh artistic style to landscape photographs. The model consists of Generator and a Discriminator"
      ],
      "metadata": {
        "id": "BENQ1EKDoB_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JNRTlX6LIdqo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "ssim_module = SSIM(data_range=1.0, size_average=True, channel=3) # channel=1 for grayscale images\n",
        "ms_ssim_module = MS_SSIM(data_range=1.0, size_average=True, channel=3)\n",
        "\n",
        "# Conv Block with two conv layers, batch norm and leaky-relu layer\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"two convolution layers with batch norm and leaky relu\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dropout_p):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_conv(x)\n",
        "\n",
        "# A downsampling module consisting of a MaxPool2d for downsampling and then a ConvBlock\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"Downsampling followed by ConvBlock\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dropout_p):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(in_channels, out_channels, dropout_p)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "# Fractionally-strided convolutions for upsampling followed by a conv block\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upssampling followed by ConvBlock\"\"\"\n",
        "    def __init__(self, in_channels1, in_channels2, out_channels, dropout_p):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.conv1x1 = nn.Conv2d(in_channels1, in_channels2, kernel_size=1)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = ConvBlock(in_channels2 * 2, out_channels, dropout_p)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.conv1x1(x1)\n",
        "        x1 = self.up(x1)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "# Defining the UNet encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.params = params\n",
        "        self.in_chns = self.params['in_chns']\n",
        "        self.ft_chns = self.params['feature_chns']\n",
        "        self.dropout = self.params['dropout']\n",
        "        assert (len(self.ft_chns) == 5)\n",
        "        self.in_conv = ConvBlock(\n",
        "            self.in_chns, self.ft_chns[0], self.dropout[0])\n",
        "        self.down1 = DownBlock(\n",
        "            self.ft_chns[0], self.ft_chns[1], self.dropout[1])\n",
        "        self.down2 = DownBlock(\n",
        "            self.ft_chns[1], self.ft_chns[2], self.dropout[2])\n",
        "        self.down3 = DownBlock(\n",
        "            self.ft_chns[2], self.ft_chns[3], self.dropout[3])\n",
        "        self.down4 = DownBlock(\n",
        "            self.ft_chns[3], self.ft_chns[4], self.dropout[4])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.in_conv(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        x4 = self.down4(x3)\n",
        "        return x4, [x0, x1, x2, x3, x4]\n",
        "\n",
        "# The Unet Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.params = params\n",
        "        self.in_chns = self.params['in_chns']\n",
        "        self.ft_chns = self.params['feature_chns']\n",
        "        assert (len(self.ft_chns) == 5)\n",
        "\n",
        "        self.up1 = UpBlock(self.ft_chns[4], self.ft_chns[3], self.ft_chns[3], dropout_p=0.0)\n",
        "        self.up2 = UpBlock(self.ft_chns[3], self.ft_chns[2], self.ft_chns[2], dropout_p=0.0)\n",
        "        self.up3 = UpBlock(self.ft_chns[2], self.ft_chns[1], self.ft_chns[1], dropout_p=0.0)\n",
        "        self.up4 = UpBlock(self.ft_chns[1], self.ft_chns[0], self.ft_chns[0], dropout_p=0.0)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(self.ft_chns[0], self.in_chns, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feature):\n",
        "        x0 = feature[0]\n",
        "        x1 = feature[1]\n",
        "        x2 = feature[2]\n",
        "        x3 = feature[3]\n",
        "        x4 = feature[4]\n",
        "\n",
        "        x = self.up1(x4, x3)\n",
        "        x = self.up2(x, x2)\n",
        "        x = self.up3(x, x1)\n",
        "        x_last = self.up4(x, x0)\n",
        "        output = self.out_conv(x_last)\n",
        "        return output, x_last\n",
        "\n",
        "\n",
        "# Combining the Encoder and the decoder to form the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        params = {'in_chns': in_channels,\n",
        "                  'feature_chns': [16, 32, 64, 128, 256],\n",
        "                  'dropout': [0.05, 0.1, 0.2, 0.3, 0.5],\n",
        "                  'acti_func': 'relu'}\n",
        "\n",
        "        self.encoder = Encoder(params)\n",
        "        self.decoder = Decoder(params)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, feature = self.encoder(x)\n",
        "        output, features = self.decoder(feature)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Conv Block for Discriminator\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,\n",
        "                      out_channels,\n",
        "                      kernel_size,\n",
        "                      stride,\n",
        "                      padding,\n",
        "                      padding_mode='reflect',\n",
        "                      bias=True),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# The dsicriminator class\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, features=(64, 128, 256, 512)):\n",
        "        super().__init__()\n",
        "        self.initial_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=features[0],\n",
        "                      kernel_size=4,\n",
        "                      stride=2,\n",
        "                      padding=1,\n",
        "                      padding_mode='reflect'),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(Block(in_channels=in_channels,\n",
        "                                out_channels=feature,\n",
        "                                kernel_size=4,\n",
        "                                stride= 1 if feature == features[-1] else 2,\n",
        "                                padding=1,\n",
        "            ))\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(nn.Conv2d(in_channels,\n",
        "                                1, 4, 1, 1, padding_mode='reflect'))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_layer(x)\n",
        "        return torch.sigmoid(self.model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell contains the Dataloader for my custom Vangogh2Photo dataset. The getitem method returns a pair of images, one from each domain\n",
        "The constructor takes as input, the root directory where the dataset is located."
      ],
      "metadata": {
        "id": "duEb3TaStV94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tO-c5DRkJTVC"
      },
      "outputs": [],
      "source": [
        "####-----------Define the dataloaders and dataset class-------------####\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class vangogh2photo(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, split='train' transform=None):\n",
        "        super().__init__()\n",
        "        self.root_vangogh = os.path.join(root_dir, f'{split}A')\n",
        "        self.root_photos = os.path.join(root_dir, f'{split}B')\n",
        "\n",
        "        self.vangogh_images = os.listdir(self.root_vangogh)\n",
        "        self.photo_images = os.listdir(self.root_photos)\n",
        "\n",
        "        self.length = max(len(self.vangogh_images), len(self.photo_images))\n",
        "        self.vangogh_len = len(self.vangogh_images)\n",
        "        self.photo_len = len(self.photo_images)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        photo_img = Image.open(os.path.join(self.root_photos, self.photo_images[index % self.photo_len])).convert('RGB')\n",
        "        vangogh_img = Image.open(os.path.join(self.root_vangogh, self.vangogh_images[index % self.vangogh_len])).convert('RGB')\n",
        "\n",
        "        photo_img = np.array(photo_img) / 255.\n",
        "        vangogh_img = np.array(vangogh_img) / 255.\n",
        "        photo_img, vangogh_img = photo_img.astype(np.float32), vangogh_img.astype(np.float32)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            photo_img = self.transform(photo_img)\n",
        "            vangogh_img = self.transform(vangogh_img)\n",
        "\n",
        "        return vangogh_img, photo_img\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TV_ToHQrWKfZ"
      },
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "import random, torch, os, numpy as np\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oIh27pWUV4so"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, Normalize, ToTensor\n",
        "\n",
        "# Hyperparameters and configs\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available else 'cpu'\n",
        "root_dir = '.'\n",
        "batch_size = 4\n",
        "lr_rate = 1e-3\n",
        "lambda_identity = 5\n",
        "lambda_cycle = 10\n",
        "num_epochs = 50\n",
        "load_model = False\n",
        "save_model = True\n",
        "monet_generator = root_dir + '/checkpoints/monet_gen.pth.tar'\n",
        "photo_generator = root_dir + '/checkpoints/photo_gen.pth.tar'\n",
        "monet_discriminator = root_dir + '/checkpoints/monet_dis.pth.tar'\n",
        "photo_discriminator = '/checkpoints/photo_dis.pth.tar'\n",
        "num_workers = 2\n",
        "transforms = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Resize(size=(256, 256)),\n",
        "        RandomHorizontalFlip(p=0.5),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell defines the training step for one epoch."
      ],
      "metadata": {
        "id": "XLpMgRKPuj_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7zLlvi0_fgFb"
      },
      "outputs": [],
      "source": [
        "## The training loop\n",
        "def train_step(disc_y, disc_x, gen_ytox, gen_xtoy, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):\n",
        "    loop = tqdm(loader)\n",
        "    y_reals = 0\n",
        "    y_fakes = 0\n",
        "    gen_loss = 0.0\n",
        "    disc_loss = 0.0\n",
        "    ssim_score, ssim_score_style = 0, 0\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        # place the two inputs on the cuda device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # First train the discriminators\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # generate fake y from x\n",
        "            fake_y = gen_xtoy(x)\n",
        "            dy_fake = disc_y(fake_y.detach())\n",
        "            dy_real = disc_y(y)\n",
        "            y_reals += dy_real.mean().item()\n",
        "            y_fakes += dy_fake.mean().item()\n",
        "            # Compute the dsicrimiator_y's loss\n",
        "            discy_real_loss = mse(dy_real, torch.ones_like(dy_real))\n",
        "            discy_fake_loss = mse(dy_fake, torch.zeros_like(dy_fake))\n",
        "            discy_loss = (discy_real_loss + discy_fake_loss) / 2\n",
        "\n",
        "            # generate fake x from y\n",
        "            fake_x = gen_ytox(y)\n",
        "            dx_fake = disc_x(fake_x.detach())\n",
        "            dx_real = disc_x(x)\n",
        "            # Compute the dsicrimiator_x's loss\n",
        "            discx_real_loss = mse(dx_real, torch.ones_like(dx_real))\n",
        "            discx_fake_loss = mse(dx_fake, torch.zeros_like(dx_fake))\n",
        "            discx_loss = (discx_real_loss + discx_fake_loss) / 2\n",
        "\n",
        "            D_loss = discy_loss + discx_loss\n",
        "        # Update the dsicriminator weights\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generators\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # 1. Adversarial Loss\n",
        "            discx_fake = disc_x(fake_x)\n",
        "            discy_fake = disc_y(fake_y)\n",
        "            loss_g_xtoy = mse(discy_fake, torch.ones_like(discy_fake))\n",
        "            loss_g_ytox = mse(discx_fake, torch.ones_like(discx_fake))\n",
        "            adv_G_loss = loss_g_ytox + loss_g_xtoy\n",
        "\n",
        "            # 2. Cycle-consistency loss\n",
        "            cycle_xtoytox = gen_ytox(fake_y)\n",
        "            cycle_ytoxtoy = gen_xtoy(fake_x)\n",
        "            cycle_x_loss = L1(cycle_xtoytox, x)\n",
        "            cycle_y_loss = L1(cycle_ytoxtoy, y)\n",
        "            cycle_G_loss = cycle_x_loss + cycle_y_loss\n",
        "\n",
        "            # 3. Identity loss\n",
        "            identity_x = gen_ytox(x)\n",
        "            identity_y = gen_xtoy(y)\n",
        "            identity_x_loss = L1(identity_x, x)\n",
        "            identity_y_loss = L1(identity_y, y)\n",
        "            identity_G_loss = identity_x_loss + identity_y_loss\n",
        "\n",
        "            # add all togethor\n",
        "            G_loss = adv_G_loss + lambda_cycle * cycle_G_loss + lambda_identity * identity_G_loss\n",
        "\n",
        "        # update generator weights\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        # save intermediate results for visualisation\n",
        "        if batch_idx % 200 == 0:\n",
        "            if not os.path.exists(root_dir + f\"/saved_images/{batch_idx}\"):\n",
        "              os.mkdir(root_dir + f\"/saved_images/{batch_idx}\")\n",
        "            save_image(fake_y, root_dir + f\"/saved_images/{batch_idx}/fake_photo.png\")\n",
        "            save_image(fake_x, root_dir + f\"/saved_images/{batch_idx}/fake_monet.png\")\n",
        "            save_image(x, root_dir + f\"/saved_images/{batch_idx}/real_monet.png\")\n",
        "            save_image(y, root_dir + f\"/saved_images/{batch_idx}/real_photo.png\")\n",
        "\n",
        "        gen_loss += G_loss.item()\n",
        "        disc_loss += D_loss.item()\n",
        "        ssim_score += 1 - ssim_module(fake_x.detach().float(), y.float()).item()\n",
        "        ssim_score_style += 1 - ssim_module(fake_x.detach().float(), x.float()).item()\n",
        "\n",
        "        # Display information on the tqdm bar\n",
        "        loop.set_postfix(y_real=y_reals / (batch_idx + 1), y_fake=y_fakes / (batch_idx + 1), gen_loss=gen_loss / (batch_idx+1), disc_loss = disc_loss/ (batch_idx+1), ssim_score=ssim_score/(batch_idx+1), style_score=ssim_score_style/(batch_idx+1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lDc9c0qZTd7",
        "outputId": "ddb61064-40d0-4fb5-d707-e010c3e52362"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:16<00:00,  4.18it/s, disc_loss=0.497, gen_loss=2.74, ssim_score=0.243, style_score=0.898, y_fake=0.458, y_real=0.529]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:07<00:00,  4.28it/s, disc_loss=0.47, gen_loss=2.53, ssim_score=0.276, style_score=0.915, y_fake=0.41, y_real=0.572]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:09<00:00,  4.26it/s, disc_loss=0.459, gen_loss=2.47, ssim_score=0.305, style_score=0.921, y_fake=0.401, y_real=0.581]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:07<00:00,  4.27it/s, disc_loss=0.444, gen_loss=2.48, ssim_score=0.326, style_score=0.924, y_fake=0.395, y_real=0.589]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:07<00:00,  4.28it/s, disc_loss=0.424, gen_loss=2.53, ssim_score=0.344, style_score=0.927, y_fake=0.383, y_real=0.599]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:08<00:00,  4.26it/s, disc_loss=0.401, gen_loss=2.58, ssim_score=0.354, style_score=0.928, y_fake=0.374, y_real=0.61]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:06<00:00,  4.29it/s, disc_loss=0.37, gen_loss=2.67, ssim_score=0.361, style_score=0.929, y_fake=0.364, y_real=0.623]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:06<00:00,  4.29it/s, disc_loss=0.338, gen_loss=2.74, ssim_score=0.36, style_score=0.929, y_fake=0.354, y_real=0.633]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:09<00:00,  4.25it/s, disc_loss=0.296, gen_loss=2.81, ssim_score=0.355, style_score=0.928, y_fake=0.343, y_real=0.642]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:04<00:00,  4.31it/s, disc_loss=0.254, gen_loss=2.88, ssim_score=0.348, style_score=0.927, y_fake=0.333, y_real=0.651]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:04<00:00,  4.32it/s, disc_loss=0.23, gen_loss=2.91, ssim_score=0.33, style_score=0.925, y_fake=0.322, y_real=0.661]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:05<00:00,  4.30it/s, disc_loss=0.212, gen_loss=2.91, ssim_score=0.315, style_score=0.924, y_fake=0.317, y_real=0.667]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:05<00:00,  4.30it/s, disc_loss=0.198, gen_loss=2.93, ssim_score=0.303, style_score=0.922, y_fake=0.305, y_real=0.676]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:06<00:00,  4.29it/s, disc_loss=0.188, gen_loss=2.91, ssim_score=0.291, style_score=0.921, y_fake=0.299, y_real=0.682]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:04<00:00,  4.31it/s, disc_loss=0.172, gen_loss=2.93, ssim_score=0.258, style_score=0.917, y_fake=0.285, y_real=0.697]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1572/1572 [06:06<00:00,  4.29it/s, disc_loss=0.175, gen_loss=2.91, ssim_score=0.254, style_score=0.917, y_fake=0.277, y_real=0.703]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [06:06<00:00,  4.29it/s, disc_loss=0.159, gen_loss=2.94, ssim_score=0.24, style_score=0.915, y_fake=0.268, y_real=0.713]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [06:03<00:00,  4.33it/s, disc_loss=0.16, gen_loss=2.92, ssim_score=0.227, style_score=0.913, y_fake=0.262, y_real=0.722]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [06:03<00:00,  4.32it/s, disc_loss=0.155, gen_loss=2.92, ssim_score=0.225, style_score=0.913, y_fake=0.259, y_real=0.727]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [06:02<00:00,  4.34it/s, disc_loss=0.148, gen_loss=2.93, ssim_score=0.207, style_score=0.911, y_fake=0.25, y_real=0.735]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1572/1572 [06:02<00:00,  4.33it/s, disc_loss=0.134, gen_loss=2.94, ssim_score=0.204, style_score=0.911, y_fake=0.235, y_real=0.75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 1188/1572 [04:33<01:24,  4.54it/s, disc_loss=0.137, gen_loss=2.94, ssim_score=0.175, style_score=0.908, y_fake=0.231, y_real=0.756]"
          ]
        }
      ],
      "source": [
        "# Define the generators and the discriminators\n",
        "disc_x = Discriminator(in_channels=3).to(device)\n",
        "disc_y = Discriminator(in_channels=3).to(device)\n",
        "gen_xtoy = Generator(in_channels=3).to(device)\n",
        "gen_ytox = Generator(in_channels=3).to(device)\n",
        "\n",
        "# Define the optimizers\n",
        "opt_disc = optim.Adam(\n",
        "    params = list(disc_x.parameters()) + list(disc_y.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "opt_gen = optim.Adam(\n",
        "    params=list(gen_xtoy.parameters()) + list(gen_ytox.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "\n",
        "# Define the loss criterion\n",
        "L1 = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "# If training a pretrained model, set load_model as True and the models will be loaded\n",
        "if load_model:\n",
        "    load_checkpoint(\n",
        "        photo_generator,\n",
        "        gen_xtoy,\n",
        "        opt_gen,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        monet_generator,\n",
        "        gen_ytox,\n",
        "        opt_gen,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        monet_discriminator,\n",
        "        disc_y,\n",
        "        opt_disc,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        photo_discriminator,\n",
        "        disc_x,\n",
        "        opt_disc,\n",
        "        lr_rate,\n",
        "    )\n",
        "\n",
        "# Create a dataset for training from the custom dataset class defined previously\n",
        "dataset = vangogh2photo(\n",
        "    root_dir=root_dir,\n",
        "    transform=transforms,\n",
        ")\n",
        "val_dataset = vangogh2photo(\n",
        "    root_dir=root_dir,\n",
        "    split='test'\n",
        "    transform=transforms,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# For mixed-precision training\n",
        "g_scaler = torch.cuda.amp.GradScaler()\n",
        "d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "if not os.path.exists(root_dir + f\"/saved_images\"):\n",
        "  os.mkdir(root_dir + f\"/saved_images\")\n",
        "\n",
        "if not os.path.exists(root_dir + f\"/checkpoints\"):\n",
        "  os.mkdir(root_dir + f\"/checkpoints\")\n",
        "\n",
        "# The training loop\n",
        "for epoch in range(num_epochs):\n",
        "    train_step(\n",
        "        disc_y,\n",
        "        disc_x,\n",
        "        gen_ytox,\n",
        "        gen_xtoy,\n",
        "        loader,\n",
        "        opt_disc,\n",
        "        opt_gen,\n",
        "        L1,\n",
        "        mse,\n",
        "        d_scaler,\n",
        "        g_scaler,\n",
        "    )\n",
        "\n",
        "    if save_model:\n",
        "        save_checkpoint(gen_xtoy, opt_gen, filename=photo_generator)\n",
        "        save_checkpoint(gen_ytox, opt_gen, filename=monet_generator)\n",
        "        save_checkpoint(disc_y, opt_disc, filename=monet_discriminator)\n",
        "        save_checkpoint(disc_x, opt_disc, filename=photo_discriminator)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bAmHLLd8yrET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWf6FrHWyrBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZQQCKKqyq-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjxiaQLlyq7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEyTfVotyq3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfaxmayHyq09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9l2CMubVyqxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahFhU9M1rP9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}