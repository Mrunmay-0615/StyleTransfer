{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pKJirIqfARAb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def _fspecial_gauss_1d(size: int, sigma: float) -> Tensor:\n",
        "    r\"\"\"Create 1-D gauss kernel\n",
        "    Args:\n",
        "        size (int): the size of gauss kernel\n",
        "        sigma (float): sigma of normal distribution\n",
        "    Returns:\n",
        "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
        "    \"\"\"\n",
        "    coords = torch.arange(size, dtype=torch.float)\n",
        "    coords -= size // 2\n",
        "\n",
        "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
        "    g /= g.sum()\n",
        "\n",
        "    return g.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "def gaussian_filter(input: Tensor, win: Tensor) -> Tensor:\n",
        "    r\"\"\" Blur input with 1-D kernel\n",
        "    Args:\n",
        "        input (torch.Tensor): a batch of tensors to be blurred\n",
        "        window (torch.Tensor): 1-D gauss kernel\n",
        "    Returns:\n",
        "        torch.Tensor: blurred tensors\n",
        "    \"\"\"\n",
        "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
        "    if len(input.shape) == 4:\n",
        "        conv = F.conv2d\n",
        "    elif len(input.shape) == 5:\n",
        "        conv = F.conv3d\n",
        "    else:\n",
        "        raise NotImplementedError(input.shape)\n",
        "\n",
        "    C = input.shape[1]\n",
        "    out = input\n",
        "    for i, s in enumerate(input.shape[2:]):\n",
        "        if s >= win.shape[-1]:\n",
        "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
        "        else:\n",
        "            warnings.warn(\n",
        "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
        "            )\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float,\n",
        "    win: Tensor,\n",
        "    size_average: bool = True,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    r\"\"\" Calculate ssim index for X and Y\n",
        "\n",
        "    Args:\n",
        "        X (torch.Tensor): images\n",
        "        Y (torch.Tensor): images\n",
        "        data_range (float or int): value range of input images. (usually 1.0 or 255)\n",
        "        win (torch.Tensor): 1-D gauss kernel\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: ssim results.\n",
        "    \"\"\"\n",
        "    K1, K2 = K\n",
        "    # batch, channel, [depth,] height, width = X.shape\n",
        "    compensation = 1.0\n",
        "\n",
        "    C1 = (K1 * data_range) ** 2\n",
        "    C2 = (K2 * data_range) ** 2\n",
        "\n",
        "    win = win.to(X.device, dtype=X.dtype)\n",
        "\n",
        "    mu1 = gaussian_filter(X, win)\n",
        "    mu2 = gaussian_filter(Y, win)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
        "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
        "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
        "\n",
        "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
        "\n",
        "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
        "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
        "    return ssim_per_channel, cs\n",
        "\n",
        "\n",
        "def ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float = 255,\n",
        "    size_average: bool = True,\n",
        "    win_size: int = 11,\n",
        "    win_sigma: float = 1.5,\n",
        "    win: Optional[Tensor] = None,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "    nonnegative_ssim: bool = False,\n",
        ") -> Tensor:\n",
        "    r\"\"\" interface of ssim\n",
        "    Args:\n",
        "        X (torch.Tensor): a batch of images, (N,C,H,W)\n",
        "        Y (torch.Tensor): a batch of images, (N,C,H,W)\n",
        "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "        win_size: (int, optional): the size of gauss kernel\n",
        "        win_sigma: (float, optional): sigma of normal distribution\n",
        "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
        "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: ssim results\n",
        "    \"\"\"\n",
        "    if not X.shape == Y.shape:\n",
        "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
        "\n",
        "    for d in range(len(X.shape) - 1, 1, -1):\n",
        "        X = X.squeeze(dim=d)\n",
        "        Y = Y.squeeze(dim=d)\n",
        "\n",
        "    if len(X.shape) not in (4, 5):\n",
        "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
        "\n",
        "    #if not X.type() == Y.type():\n",
        "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
        "\n",
        "    if win is not None:  # set win_size\n",
        "        win_size = win.shape[-1]\n",
        "\n",
        "    if not (win_size % 2 == 1):\n",
        "        raise ValueError(\"Window size should be odd.\")\n",
        "\n",
        "    if win is None:\n",
        "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
        "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
        "\n",
        "    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n",
        "    if nonnegative_ssim:\n",
        "        ssim_per_channel = torch.relu(ssim_per_channel)\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_per_channel.mean()\n",
        "    else:\n",
        "        return ssim_per_channel.mean(1)\n",
        "\n",
        "\n",
        "def ms_ssim(\n",
        "    X: Tensor,\n",
        "    Y: Tensor,\n",
        "    data_range: float = 255,\n",
        "    size_average: bool = True,\n",
        "    win_size: int = 11,\n",
        "    win_sigma: float = 1.5,\n",
        "    win: Optional[Tensor] = None,\n",
        "    weights: Optional[List[float]] = None,\n",
        "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
        ") -> Tensor:\n",
        "    r\"\"\" interface of ms-ssim\n",
        "    Args:\n",
        "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
        "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
        "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "        win_size: (int, optional): the size of gauss kernel\n",
        "        win_sigma: (float, optional): sigma of normal distribution\n",
        "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
        "        weights (list, optional): weights for different levels\n",
        "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "    Returns:\n",
        "        torch.Tensor: ms-ssim results\n",
        "    \"\"\"\n",
        "    if not X.shape == Y.shape:\n",
        "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
        "\n",
        "    for d in range(len(X.shape) - 1, 1, -1):\n",
        "        X = X.squeeze(dim=d)\n",
        "        Y = Y.squeeze(dim=d)\n",
        "\n",
        "    #if not X.type() == Y.type():\n",
        "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
        "\n",
        "    if len(X.shape) == 4:\n",
        "        avg_pool = F.avg_pool2d\n",
        "    elif len(X.shape) == 5:\n",
        "        avg_pool = F.avg_pool3d\n",
        "    else:\n",
        "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
        "\n",
        "    if win is not None:  # set win_size\n",
        "        win_size = win.shape[-1]\n",
        "\n",
        "    if not (win_size % 2 == 1):\n",
        "        raise ValueError(\"Window size should be odd.\")\n",
        "\n",
        "    smaller_side = min(X.shape[-2:])\n",
        "    assert smaller_side > (win_size - 1) * (\n",
        "        2 ** 4\n",
        "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
        "\n",
        "    if weights is None:\n",
        "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
        "    weights_tensor = X.new_tensor(weights)\n",
        "\n",
        "    if win is None:\n",
        "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
        "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
        "\n",
        "    levels = weights_tensor.shape[0]\n",
        "    mcs = []\n",
        "    for i in range(levels):\n",
        "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n",
        "\n",
        "        if i < levels - 1:\n",
        "            mcs.append(torch.relu(cs))\n",
        "            padding = [s % 2 for s in X.shape[2:]]\n",
        "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
        "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
        "\n",
        "    ssim_per_channel = torch.relu(ssim_per_channel)  # type: ignore  # (batch, channel)\n",
        "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
        "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights_tensor.view(-1, 1, 1), dim=0)\n",
        "\n",
        "    if size_average:\n",
        "        return ms_ssim_val.mean()\n",
        "    else:\n",
        "        return ms_ssim_val.mean(1)\n",
        "\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_range: float = 255,\n",
        "        size_average: bool = True,\n",
        "        win_size: int = 11,\n",
        "        win_sigma: float = 1.5,\n",
        "        channel: int = 3,\n",
        "        spatial_dims: int = 2,\n",
        "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "        nonnegative_ssim: bool = False,\n",
        "    ) -> None:\n",
        "        r\"\"\" class for ssim\n",
        "        Args:\n",
        "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "            win_size: (int, optional): the size of gauss kernel\n",
        "            win_sigma: (float, optional): sigma of normal distribution\n",
        "            channel (int, optional): input channels (default: 3)\n",
        "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n",
        "        \"\"\"\n",
        "\n",
        "        super(SSIM, self).__init__()\n",
        "        self.win_size = win_size\n",
        "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.K = K\n",
        "        self.nonnegative_ssim = nonnegative_ssim\n",
        "\n",
        "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
        "        return ssim(\n",
        "            X,\n",
        "            Y,\n",
        "            data_range=self.data_range,\n",
        "            size_average=self.size_average,\n",
        "            win=self.win,\n",
        "            K=self.K,\n",
        "            nonnegative_ssim=self.nonnegative_ssim,\n",
        "        )\n",
        "\n",
        "\n",
        "class MS_SSIM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_range: float = 255,\n",
        "        size_average: bool = True,\n",
        "        win_size: int = 11,\n",
        "        win_sigma: float = 1.5,\n",
        "        channel: int = 3,\n",
        "        spatial_dims: int = 2,\n",
        "        weights: Optional[List[float]] = None,\n",
        "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
        "    ) -> None:\n",
        "        r\"\"\" class for ms-ssim\n",
        "        Args:\n",
        "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
        "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
        "            win_size: (int, optional): the size of gauss kernel\n",
        "            win_sigma: (float, optional): sigma of normal distribution\n",
        "            channel (int, optional): input channels (default: 3)\n",
        "            weights (list, optional): weights for different levels\n",
        "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
        "        \"\"\"\n",
        "\n",
        "        super(MS_SSIM, self).__init__()\n",
        "        self.win_size = win_size\n",
        "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
        "        self.size_average = size_average\n",
        "        self.data_range = data_range\n",
        "        self.weights = weights\n",
        "        self.K = K\n",
        "\n",
        "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
        "        return ms_ssim(\n",
        "            X,\n",
        "            Y,\n",
        "            data_range=self.data_range,\n",
        "            size_average=self.size_average,\n",
        "            win=self.win,\n",
        "            weights=self.weights,\n",
        "            K=self.K,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BENQ1EKDoB_N"
      },
      "source": [
        "### Defining the model architecture.\n",
        "\n",
        "As stated in the Readme.md, I am using CycleGAN (Zhu et. al, 2017) for transferring the Vangogh artistic style to landscape photographs. The model consists of Generator and a Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JNRTlX6LIdqo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "ssim_module = SSIM(data_range=1.0, size_average=True, channel=3) # channel=1 for grayscale images\n",
        "ms_ssim_module = MS_SSIM(data_range=1.0, size_average=True, channel=3)\n",
        "\n",
        "# Conv Block with two conv layers, batch norm and leaky-relu layer\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"two convolution layers with batch norm and leaky relu\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dropout_p):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_conv(x)\n",
        "\n",
        "# A downsampling module consisting of a MaxPool2d for downsampling and then a ConvBlock\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"Downsampling followed by ConvBlock\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, dropout_p):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            ConvBlock(in_channels, out_channels, dropout_p)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "# Fractionally-strided convolutions for upsampling followed by a conv block\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upssampling followed by ConvBlock\"\"\"\n",
        "    def __init__(self, in_channels1, in_channels2, out_channels, dropout_p):\n",
        "        super(UpBlock, self).__init__()\n",
        "        self.conv1x1 = nn.Conv2d(in_channels1, in_channels2, kernel_size=1)\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = ConvBlock(in_channels2 * 2, out_channels, dropout_p)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.conv1x1(x1)\n",
        "        x1 = self.up(x1)\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "# Defining the UNet encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.params = params\n",
        "        self.in_chns = self.params['in_chns']\n",
        "        self.ft_chns = self.params['feature_chns']\n",
        "        self.dropout = self.params['dropout']\n",
        "        assert (len(self.ft_chns) == 5)\n",
        "        self.in_conv = ConvBlock(\n",
        "            self.in_chns, self.ft_chns[0], self.dropout[0])\n",
        "        self.down1 = DownBlock(\n",
        "            self.ft_chns[0], self.ft_chns[1], self.dropout[1])\n",
        "        self.down2 = DownBlock(\n",
        "            self.ft_chns[1], self.ft_chns[2], self.dropout[2])\n",
        "        self.down3 = DownBlock(\n",
        "            self.ft_chns[2], self.ft_chns[3], self.dropout[3])\n",
        "        self.down4 = DownBlock(\n",
        "            self.ft_chns[3], self.ft_chns[4], self.dropout[4])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.in_conv(x)\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        x4 = self.down4(x3)\n",
        "        return x4, [x0, x1, x2, x3, x4]\n",
        "\n",
        "# The Unet Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.params = params\n",
        "        self.in_chns = self.params['in_chns']\n",
        "        self.ft_chns = self.params['feature_chns']\n",
        "        assert (len(self.ft_chns) == 5)\n",
        "\n",
        "        self.up1 = UpBlock(self.ft_chns[4], self.ft_chns[3], self.ft_chns[3], dropout_p=0.0)\n",
        "        self.up2 = UpBlock(self.ft_chns[3], self.ft_chns[2], self.ft_chns[2], dropout_p=0.0)\n",
        "        self.up3 = UpBlock(self.ft_chns[2], self.ft_chns[1], self.ft_chns[1], dropout_p=0.0)\n",
        "        self.up4 = UpBlock(self.ft_chns[1], self.ft_chns[0], self.ft_chns[0], dropout_p=0.0)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(self.ft_chns[0], self.in_chns, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, feature):\n",
        "        x0 = feature[0]\n",
        "        x1 = feature[1]\n",
        "        x2 = feature[2]\n",
        "        x3 = feature[3]\n",
        "        x4 = feature[4]\n",
        "\n",
        "        x = self.up1(x4, x3)\n",
        "        x = self.up2(x, x2)\n",
        "        x = self.up3(x, x1)\n",
        "        x_last = self.up4(x, x0)\n",
        "        output = self.out_conv(x_last)\n",
        "        return output, x_last\n",
        "\n",
        "\n",
        "# Combining the Encoder and the decoder to form the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        params = {'in_chns': in_channels,\n",
        "                  'feature_chns': [16, 32, 64, 128, 256],\n",
        "                  'dropout': [0.05, 0.1, 0.2, 0.3, 0.5],\n",
        "                  'acti_func': 'relu'}\n",
        "\n",
        "        self.encoder = Encoder(params)\n",
        "        self.decoder = Decoder(params)\n",
        "\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        _, feature = self.encoder(x)\n",
        "        output, features = self.decoder(feature)\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "# Conv Block for Discriminator\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, padding=1):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels,\n",
        "                      out_channels,\n",
        "                      kernel_size,\n",
        "                      stride,\n",
        "                      padding,\n",
        "                      padding_mode='reflect',\n",
        "                      bias=True),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# The dsicriminator class\n",
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, features=(64, 128, 256, 512)):\n",
        "        super().__init__()\n",
        "        self.initial_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=features[0],\n",
        "                      kernel_size=4,\n",
        "                      stride=2,\n",
        "                      padding=1,\n",
        "                      padding_mode='reflect'),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(Block(in_channels=in_channels,\n",
        "                                out_channels=feature,\n",
        "                                kernel_size=4,\n",
        "                                stride= 1 if feature == features[-1] else 2,\n",
        "                                padding=1,\n",
        "            ))\n",
        "            in_channels = feature\n",
        "\n",
        "        layers.append(nn.Conv2d(in_channels,\n",
        "                                1, 4, 1, 1, padding_mode='reflect'))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial_layer(x)\n",
        "        return torch.sigmoid(self.model(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duEb3TaStV94"
      },
      "source": [
        "The following cell contains the Dataloader for my custom Vangogh2Photo dataset. The getitem method returns a pair of images, one from each domain\n",
        "The constructor takes as input, the root directory where the dataset is located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tO-c5DRkJTVC"
      },
      "outputs": [],
      "source": [
        "####-----------Define the dataloaders and dataset class-------------####\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class vangogh2photo(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        super().__init__()\n",
        "        self.root_vangogh = os.path.join(root_dir, f'{split}A')\n",
        "        self.root_photos = os.path.join(root_dir, f'{split}B')\n",
        "\n",
        "        self.vangogh_images = os.listdir(self.root_vangogh)\n",
        "        self.photo_images = os.listdir(self.root_photos)\n",
        "\n",
        "        self.length = max(len(self.vangogh_images), len(self.photo_images))\n",
        "        self.vangogh_len = len(self.vangogh_images)\n",
        "        self.photo_len = len(self.photo_images)\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        photo_img = Image.open(os.path.join(self.root_photos, self.photo_images[index % self.photo_len])).convert('RGB')\n",
        "        vangogh_img = Image.open(os.path.join(self.root_vangogh, self.vangogh_images[index % self.vangogh_len])).convert('RGB')\n",
        "\n",
        "        photo_img = np.array(photo_img) / 255.\n",
        "        vangogh_img = np.array(vangogh_img) / 255.\n",
        "        photo_img, vangogh_img = photo_img.astype(np.float32), vangogh_img.astype(np.float32)\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            photo_img = self.transform(photo_img)\n",
        "            vangogh_img = self.transform(vangogh_img)\n",
        "\n",
        "        return vangogh_img, photo_img\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TV_ToHQrWKfZ"
      },
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "import random, torch, os, numpy as np\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oIh27pWUV4so"
      },
      "outputs": [],
      "source": [
        "# train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.transforms import Compose, Resize, RandomHorizontalFlip, Normalize, ToTensor\n",
        "\n",
        "# Hyperparameters and configs\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available else 'cpu'\n",
        "root_dir = '.'\n",
        "batch_size = 4\n",
        "lr_rate = 1e-3\n",
        "lambda_identity = 5\n",
        "lambda_cycle = 10\n",
        "num_epochs = 50\n",
        "load_model = False\n",
        "save_model = True\n",
        "vangogh_generator = root_dir + '/checkpoints/vangogh_gen.pth.tar'\n",
        "photo_generator = root_dir + '/checkpoints/photo_gen.pth.tar'\n",
        "vangogh_discriminator = root_dir + '/checkpoints/vangogh_dis.pth.tar'\n",
        "photo_discriminator = '/checkpoints/photo_dis.pth.tar'\n",
        "num_workers = 2\n",
        "transforms = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Resize(size=(256, 256)),\n",
        "        RandomHorizontalFlip(p=0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        ToTensor(),\n",
        "        Resize(size=(256, 256)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLpMgRKPuj_8"
      },
      "source": [
        "The following cell defines the training step for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7zLlvi0_fgFb"
      },
      "outputs": [],
      "source": [
        "## The training loop\n",
        "def train_step(disc_y, disc_x, gen_ytox, gen_xtoy, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):\n",
        "    loop = tqdm(loader)\n",
        "    y_reals = 0\n",
        "    y_fakes = 0\n",
        "    gen_loss = 0.0\n",
        "    disc_loss = 0.0\n",
        "    ssim_score, ssim_score_style = 0, 0\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        # place the two inputs on the cuda device\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # First train the discriminators\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # generate fake y from x\n",
        "            fake_y = gen_xtoy(x)\n",
        "            dy_fake = disc_y(fake_y.detach())\n",
        "            dy_real = disc_y(y)\n",
        "            y_reals += dy_real.mean().item()\n",
        "            y_fakes += dy_fake.mean().item()\n",
        "            # Compute the dsicrimiator_y's loss\n",
        "            discy_real_loss = mse(dy_real, torch.ones_like(dy_real))\n",
        "            discy_fake_loss = mse(dy_fake, torch.zeros_like(dy_fake))\n",
        "            discy_loss = (discy_real_loss + discy_fake_loss) / 2\n",
        "\n",
        "            # generate fake x from y\n",
        "            fake_x = gen_ytox(y)\n",
        "            dx_fake = disc_x(fake_x.detach())\n",
        "            dx_real = disc_x(x)\n",
        "            # Compute the dsicrimiator_x's loss\n",
        "            discx_real_loss = mse(dx_real, torch.ones_like(dx_real))\n",
        "            discx_fake_loss = mse(dx_fake, torch.zeros_like(dx_fake))\n",
        "            discx_loss = (discx_real_loss + discx_fake_loss) / 2\n",
        "\n",
        "            D_loss = discy_loss + discx_loss\n",
        "        # Update the dsicriminator weights\n",
        "        opt_disc.zero_grad()\n",
        "        d_scaler.scale(D_loss).backward()\n",
        "        d_scaler.step(opt_disc)\n",
        "        d_scaler.update()\n",
        "\n",
        "        # Train generators\n",
        "        with torch.cuda.amp.autocast():\n",
        "            # 1. Adversarial Loss\n",
        "            discx_fake = disc_x(fake_x)\n",
        "            discy_fake = disc_y(fake_y)\n",
        "            loss_g_xtoy = mse(discy_fake, torch.ones_like(discy_fake))\n",
        "            loss_g_ytox = mse(discx_fake, torch.ones_like(discx_fake))\n",
        "            adv_G_loss = loss_g_ytox + loss_g_xtoy\n",
        "\n",
        "            # 2. Cycle-consistency loss\n",
        "            cycle_xtoytox = gen_ytox(fake_y)\n",
        "            cycle_ytoxtoy = gen_xtoy(fake_x)\n",
        "            cycle_x_loss = L1(cycle_xtoytox, x)\n",
        "            cycle_y_loss = L1(cycle_ytoxtoy, y)\n",
        "            cycle_G_loss = cycle_x_loss + cycle_y_loss\n",
        "\n",
        "            # 3. Identity loss\n",
        "            identity_x = gen_ytox(x)\n",
        "            identity_y = gen_xtoy(y)\n",
        "            identity_x_loss = L1(identity_x, x)\n",
        "            identity_y_loss = L1(identity_y, y)\n",
        "            identity_G_loss = identity_x_loss + identity_y_loss\n",
        "\n",
        "            # add all togethor\n",
        "            G_loss = adv_G_loss + lambda_cycle * cycle_G_loss + lambda_identity * identity_G_loss\n",
        "\n",
        "        # update generator weights\n",
        "        opt_gen.zero_grad()\n",
        "        g_scaler.scale(G_loss).backward()\n",
        "        g_scaler.step(opt_gen)\n",
        "        g_scaler.update()\n",
        "\n",
        "        # save intermediate results for visualisation\n",
        "        if batch_idx % 200 == 0:\n",
        "            if not os.path.exists(root_dir + f\"/saved_images/{batch_idx}\"):\n",
        "              os.mkdir(root_dir + f\"/saved_images/{batch_idx}\")\n",
        "            save_image(fake_y, root_dir + f\"/saved_images/{batch_idx}/fake_photo.png\")\n",
        "            save_image(fake_x, root_dir + f\"/saved_images/{batch_idx}/fake_vangogh.png\")\n",
        "            save_image(x, root_dir + f\"/saved_images/{batch_idx}/real_vangogh.png\")\n",
        "            save_image(y, root_dir + f\"/saved_images/{batch_idx}/real_photo.png\")\n",
        "\n",
        "        gen_loss += G_loss.item()\n",
        "        disc_loss += D_loss.item()\n",
        "        ssim_score += 1 - ssim_module(fake_x.detach().float(), y.float()).item()\n",
        "        ssim_score_style += 1 - ssim_module(fake_x.detach().float(), x.float()).item()\n",
        "\n",
        "        # Display information on the tqdm bar\n",
        "        loop.set_postfix(y_real=y_reals / (batch_idx + 1), y_fake=y_fakes / (batch_idx + 1), gen_loss=gen_loss / (batch_idx+1), disc_loss = disc_loss/ (batch_idx+1), ssim_score=ssim_score/(batch_idx+1), style_score=ssim_score_style/(batch_idx+1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lDc9c0qZTd7",
        "outputId": "ddb61064-40d0-4fb5-d707-e010c3e52362"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './trainA'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     load_checkpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         photo_discriminator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         disc_x,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         opt_disc,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         lr_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Create a dataset for training from the custom dataset class defined previously\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m dataset \u001b[39m=\u001b[39m vangogh2photo(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     root_dir\u001b[39m=\u001b[39;49mroot_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     transform\u001b[39m=\u001b[39;49mtransforms,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m val_dataset \u001b[39m=\u001b[39m vangogh2photo(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     root_dir\u001b[39m=\u001b[39mroot_dir,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     split\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     transform\u001b[39m=\u001b[39mval_transforms,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     val_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m )\n",
            "\u001b[1;32m/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_vangogh \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_photos \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root_dir, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvangogh_images \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_vangogh)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mphoto_images \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_photos)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mrunmay/Desktop/Mrunmay/Pytorch/GANs/Dashtoon/main.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvangogh_images), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mphoto_images))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './trainA'"
          ]
        }
      ],
      "source": [
        "# Define the generators and the discriminators\n",
        "disc_x = Discriminator(in_channels=3).to(device)\n",
        "disc_y = Discriminator(in_channels=3).to(device)\n",
        "gen_xtoy = Generator(in_channels=3).to(device)\n",
        "gen_ytox = Generator(in_channels=3).to(device)\n",
        "\n",
        "# Define the optimizers\n",
        "opt_disc = optim.Adam(\n",
        "    params = list(disc_x.parameters()) + list(disc_y.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "opt_gen = optim.Adam(\n",
        "    params=list(gen_xtoy.parameters()) + list(gen_ytox.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "\n",
        "# Define the loss criterion\n",
        "L1 = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "# If training a pretrained model, set load_model as True and the models will be loaded\n",
        "if load_model:\n",
        "    load_checkpoint(\n",
        "        photo_generator,\n",
        "        gen_xtoy,\n",
        "        opt_gen,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        vangogh_generator,\n",
        "        gen_ytox,\n",
        "        opt_gen,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        vangogh_discriminator,\n",
        "        disc_y,\n",
        "        opt_disc,\n",
        "        lr_rate,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        photo_discriminator,\n",
        "        disc_x,\n",
        "        opt_disc,\n",
        "        lr_rate,\n",
        "    )\n",
        "\n",
        "# Create a dataset for training from the custom dataset class defined previously\n",
        "dataset = vangogh2photo(\n",
        "    root_dir=root_dir+'vangogh2photo',\n",
        "    transform=transforms,\n",
        ")\n",
        "val_dataset = vangogh2photo(\n",
        "    root_dir=root_dir+'vangogh2photo',\n",
        "    split='test',\n",
        "    transform=val_transforms,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# # For mixed-precision training\n",
        "# g_scaler = torch.cuda.amp.GradScaler()\n",
        "# d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# if not os.path.exists(root_dir + f\"/saved_images\"):\n",
        "#   os.mkdir(root_dir + f\"/saved_images\")\n",
        "\n",
        "# if not os.path.exists(root_dir + f\"/checkpoints\"):\n",
        "#   os.mkdir(root_dir + f\"/checkpoints\")\n",
        "\n",
        "# # The training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     train_step(\n",
        "#         disc_y,\n",
        "#         disc_x,\n",
        "#         gen_ytox,\n",
        "#         gen_xtoy,\n",
        "#         loader,\n",
        "#         opt_disc,\n",
        "#         opt_gen,\n",
        "#         L1,\n",
        "#         mse,\n",
        "#         d_scaler,\n",
        "#         g_scaler,\n",
        "#     )\n",
        "\n",
        "#     if save_model:\n",
        "#         save_checkpoint(gen_xtoy, opt_gen, filename=photo_generator)\n",
        "#         save_checkpoint(gen_ytox, opt_gen, filename=vangogh_generator)\n",
        "#         save_checkpoint(disc_y, opt_disc, filename=vangogh_discriminator)\n",
        "#         save_checkpoint(disc_x, opt_disc, filename=photo_discriminator)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I define the inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAmHLLd8yrET"
      },
      "outputs": [],
      "source": [
        "# get the test loaders and load the models\n",
        "disc_x = Discriminator(in_channels=3).to(device)\n",
        "disc_y = Discriminator(in_channels=3).to(device)\n",
        "gen_xtoy = Generator(in_channels=3).to(device)\n",
        "gen_ytox = Generator(in_channels=3).to(device)\n",
        "\n",
        "# Define the optimizers\n",
        "opt_disc = optim.Adam(\n",
        "    params = list(disc_x.parameters()) + list(disc_y.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "opt_gen = optim.Adam(\n",
        "    params=list(gen_xtoy.parameters()) + list(gen_ytox.parameters()),\n",
        "    lr=lr_rate,\n",
        "    betas=(0.5, 0.999)\n",
        ")\n",
        "\n",
        "vangogh_discriminator = './checkpoints/vangogh_dis.pth.tar'\n",
        "vangogh_generator = './checkpoints/vangogh_gen.pth.tar'\n",
        "photo_discriminator = './checkpoints/photo_gen.pth.tar'\n",
        "photo_discriminator = './checkpoints/photo_gen.pth.tar'\n",
        "\n",
        "load_checkpoint(\n",
        "        photo_generator,\n",
        "        gen_xtoy,\n",
        "        opt_gen,\n",
        "        lr_rate,\n",
        ")\n",
        "load_checkpoint(\n",
        "    vangogh_generator,\n",
        "    gen_ytox,\n",
        "    opt_gen,\n",
        "    lr_rate,\n",
        ")\n",
        "load_checkpoint(\n",
        "    vangogh_discriminator,\n",
        "    disc_y,\n",
        "    opt_disc,\n",
        "    lr_rate,\n",
        ")\n",
        "load_checkpoint(\n",
        "    photo_discriminator,\n",
        "    disc_x,\n",
        "    opt_disc,\n",
        "    lr_rate,\n",
        ")\n",
        "\n",
        "\n",
        "# Initialise the ssim module for evaluating content preservation\n",
        "ssim_module = SSIM(data_range=1.0, channel=3)\n",
        "\n",
        "loop = tqdm(val_loader)\n",
        "y_reals = y_fake = 0\n",
        "for batch_idx, (x, y) in enumerate(loop):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_x = gen_ytox(y)\n",
        "        dy_fake = disc_y(fake_x)\n",
        "        dy_real = disc_y(y)\n",
        "        print(dy_fake.shape)\n",
        "        print(dy_real.shape)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWf6FrHWyrBV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZQQCKKqyq-S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjxiaQLlyq7D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEyTfVotyq3-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfaxmayHyq09"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l2CMubVyqxV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahFhU9M1rP9N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
